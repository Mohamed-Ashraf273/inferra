import io

import soundfile as sf
import torch
import torch.nn as nn
import torchaudio.transforms as transforms
from fastapi import FastAPI
from fastapi import UploadFile

from inferra.src.models.au57 import Au57  # your custom class

device = torch.device("cpu")
model = Au57(num_classes=50).to(device)
model.load_state_dict(torch.load("au57_weights.pth", map_location=device))
model.eval()

idx_to_class = {
    0: "airplane âœˆï¸",
    1: "breathing ðŸ˜®â€ðŸ’¨",
    2: "brushing_teeth ðŸª¥",
    3: "can_opening ðŸ¥«",
    4: "car_horn ðŸš—ðŸ“¢",
    5: "cat ðŸ±",
    6: "chainsaw ðŸªš",
    7: "chirping_birds ðŸ¦",
    8: "church_bells ðŸ””",
    9: "clapping ðŸ‘",
    10: "clock_alarm â°",
    11: "clock_tick ðŸ•°ï¸",
    12: "coughing ðŸ¤§",
    13: "cow ðŸ„",
    14: "crackling_fire ðŸ”¥",
    15: "crickets ðŸ¦—",
    16: "crow ðŸª¶",
    17: "crying_baby ðŸ‘¶ðŸ˜­",
    18: "dog ðŸ¶",
    19: "door_wood_creaks ðŸšª",
    20: "door_wood_knock ðŸšªðŸ”¨",
    21: "drinking_sipping ðŸ¥¤",
    22: "engine ðŸŽï¸",
    23: "fireworks ðŸŽ†",
    24: "footsteps ðŸ‘£",
    25: "frog ðŸ¸",
    26: "glass_breaking ðŸ¥‚ðŸ’¥",
    27: "hand_saw ðŸª“",
    28: "helicopter ðŸš",
    29: "hen ðŸ”",
    30: "insects ðŸž",
    31: "keyboard_typing âŒ¨ï¸",
    32: "laughing ðŸ˜‚",
    33: "mouse_click ðŸ–±ï¸",
    34: "pig ðŸ·",
    35: "pouring_water ðŸ’§",
    36: "rain ðŸŒ§ï¸",
    37: "rooster ðŸ“",
    38: "sea_waves ðŸŒŠ",
    39: "sheep ðŸ‘",
    40: "siren ðŸš¨",
    41: "sneezing ðŸ¤§",
    42: "snoring ðŸ˜´",
    43: "thunderstorm â›ˆï¸",
    44: "toilet_flush ðŸš½",
    45: "train ðŸš†",
    46: "vacuum_cleaner ðŸ§¹",
    47: "washing_machine ðŸ§º",
    48: "water_drops ðŸ’¦",
    49: "wind ðŸŒ¬ï¸",
}

transform = nn.Sequential(
    transforms.MelSpectrogram(
        sample_rate=22050,
        n_fft=1024,
        hop_length=512,
        n_mels=128,
        f_min=0,
        f_max=11025,
    ),
    transforms.AmplitudeToDB(),
)

app = FastAPI(title="Au57 Sound Classifier API")


def preprocess(waveform):
    if waveform.shape[0] > 1:
        waveform = torch.mean(waveform, dim=0, keepdim=True)
    spectrogram = transform(waveform)
    return spectrogram.unsqueeze(0).to(device)


@app.post("/predict")
async def predict(file: UploadFile):
    contents = await file.read()
    audio_buffer = io.BytesIO(contents)

    waveform, sr = sf.read(audio_buffer, dtype="float32")
    waveform = torch.tensor(waveform).T  # [channels, samples]

    inputs = preprocess(waveform)

    with torch.no_grad():
        logits = model(inputs)
        probs = torch.softmax(logits, dim=1).cpu().numpy().tolist()
        pred_idx = int(torch.argmax(logits, dim=1).item())
        pred_class = idx_to_class[pred_idx]

    return {"pred_class": pred_class, "pred_vector": probs}
